{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malehzja/lis4693/blob/main/lab-2/Lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Text Pre-Processing using spaCy\n",
        "\n",
        "In this lab assignment, we will learn to perform some basic text pre-processing using spaCy.\n",
        "\n",
        "*Note: Before starting this lab assignment, please complete the Introduction to spaCy notebook*\n",
        "\n",
        "## Leaning Objectives\n",
        "\n",
        "In this exercise, you will:\n",
        "\n",
        "- Load and process your own text file (transcript.txt)\n",
        "- Split text into sentences\n",
        "- Count words and sentences\n",
        "- Find frequently used words\n",
        "- Use spaCy’s PhraseMatcher to find specific phrases\n",
        "- Understand the difference between blank pipelines and full pipelines\n",
        "\n",
        "This exercise builds directly on concepts from discussed in the precursor notebook on \"Introduction to spaCy\"."
      ],
      "metadata": {
        "id": "KrmsSk1Yzxjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install spaCy"
      ],
      "metadata": {
        "id": "nn-U5ftJ0ebE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zOaYrB6szuCR",
        "outputId": "765da8a7-63d2-41a2-8113-9dd78f4a5cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.24.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (26.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (0.24.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.1.1)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "nf4NfFdQ0vQH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing and installing spaCy"
      ],
      "metadata": {
        "id": "VFQUMhwoK7j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and read your file from GitHub\n"
      ],
      "metadata": {
        "id": "e4aSZBw50lIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/malehzja/lis4693/refs/heads/main/lab-1/transcript-2.txt\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status() # Raise an exception for HTTP errors\n",
        "text = response.text"
      ],
      "metadata": {
        "id": "FmwSet870txZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of characters:\", len(text)) # print number of characters\n",
        "\n",
        "print(text[:100])   # print first 100 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5D5nv0Z1Irp",
        "outputId": "3dba8b38-dd15-4fd8-969a-350726d60d83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters: 6029\n",
            "Should\n",
            "I get the laundry going first?\n",
            "Hello everyone and welcome back to my\n",
            "channel. I'm going to do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 1: Load and read your transcript.txt file from lab-1 from GitHub repo directly**\n",
        "\n",
        "*NOTE: My initial transcript was in Korean so I made an English one and named it transcript-2.*"
      ],
      "metadata": {
        "id": "Pak_fIVNL5Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2: How many characters are in your transcript? Print the first 100 characters.**  \n",
        "6029 characters in total."
      ],
      "metadata": {
        "id": "E2vUXMO0LCS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Segmentation\n",
        "\n",
        "Create a blank spaCy model and add sentencizer."
      ],
      "metadata": {
        "id": "5NkXcTeY2HEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "sentences = list(doc.sents)\n",
        "\n",
        "print(\"Number of sentences:\", len(sentences))\n",
        "\n",
        "print(\"\\nFirst 3 sentences:\")\n",
        "for sent in sentences[:3]:\n",
        "    print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58uljdmW2OWY",
        "outputId": "634b1de9-194a-4e9d-f8ca-8e734516742e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 124\n",
            "\n",
            "First 3 sentences:\n",
            "Should\n",
            "I get the laundry going first?\n",
            "\n",
            "Hello everyone and welcome back to my\n",
            "channel.\n",
            "I'm going to do a Sunday reset\n",
            "because I've been holding off a lot of\n",
            "cleaning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 3: Perform sentence segmentation using the blank pipeline**\n",
        "\n",
        "How many sentences are in your transcript?  \n",
        "124.\n",
        "\n",
        "Print the first 3 sentences.  \n",
        "*NOTE: I changed from printing the first 5 to 3.*"
      ],
      "metadata": {
        "id": "UXiqIMnIMgNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Count and Token Analysis\n",
        "\n",
        "Let's count total words and unique words in your text file."
      ],
      "metadata": {
        "id": "gYBJagaG2XLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [token.text.lower() for token in doc if token.is_alpha]\n",
        "\n",
        "print(\"Total words:\", len(words))\n",
        "print(\"Unique words:\", len(set(words)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KuPcIi62dU-",
        "outputId": "dff957fb-02c0-4899-e1ca-77f172cbb484"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 1178\n",
            "Unique words: 399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 4: Perform Word Count and Token Analysis**\n",
        "\n",
        "What is the total number of words?  \n",
        "1178.\n",
        "\n",
        "What is the number of unique words?   \n",
        "399."
      ],
      "metadata": {
        "id": "jEGWZFlfNq8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most Frequent Words\n",
        "\n",
        "Let's find top 10 most frequent words in your file."
      ],
      "metadata": {
        "id": "Rl8_kxnb2kIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_freq = Counter(words)\n",
        "\n",
        "print(\"Top 10 most frequent words:\")\n",
        "for word, count in word_freq.most_common(10):\n",
        "    print(word, count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCiWP90v2rFZ",
        "outputId": "64ee8b1d-4f60-403e-a8ab-1d9ec0f822a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent words:\n",
            "i 54\n",
            "the 43\n",
            "to 34\n",
            "you 34\n",
            "a 33\n",
            "it 32\n",
            "this 30\n",
            "and 26\n",
            "so 26\n",
            "is 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 5: Find Most Frequent Words**\n",
        "\n",
        "What is the most frequent word?   \n",
        "The most frquent word is \"i\".\n",
        "\n",
        "Why do you think this word appears frequently?    \n",
        "I think the word \"i\" appears most frequently because Michelle Choi is speaking over the video in first-person narrative. It is about HER and a day in HER life."
      ],
      "metadata": {
        "id": "BcFpmCTeOdxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Full spaCy Pipeline\n",
        "\n",
        "Now use the full model for better linguistic analysis."
      ],
      "metadata": {
        "id": "pwWY7k2q3E19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp2 = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc2 = nlp2(text)\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc2.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "print(\"Total Named Entities:\", len(doc2.ents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLbCpPFk3KRw",
        "outputId": "55e9f8a4-2593-4f2f-895c-890a2522a253"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "first ORDINAL\n",
            "Sunday DATE\n",
            "the day DATE\n",
            "Dawn PERSON\n",
            "Korea GPE\n",
            "Amazon ORG\n",
            "one CARDINAL\n",
            "POV ORG\n",
            "the Insta 360 Go\n",
            "Ultra LAW\n",
            "4K QUANTITY\n",
            "POV ORG\n",
            "POV ORG\n",
            "the Insta 360 Go Ultra LAW\n",
            "POV ORG\n",
            "Valentine PERSON\n",
            "Michelle\n",
            "Choy PERSON\n",
            "15% PERCENT\n",
            "Today DATE\n",
            "Can This Love Be Translated WORK_OF_ART\n",
            "Today DATE\n",
            "New York GPE\n",
            "Korea GPE\n",
            "nitpicky GPE\n",
            "Blair Osman PERSON\n",
            "one CARDINAL\n",
            "Japan GPE\n",
            "Korea GPE\n",
            "Toyota ORG\n",
            "the Kia Carnival FAC\n",
            "a third row DATE\n",
            "Kia ORG\n",
            "Talib ORG\n",
            "POV ORG\n",
            "Ago Ultra PERSON\n",
            "Michelle Choy PERSON\n",
            "15% PERCENT\n",
            "Total Named Entities: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 6: Run full spaCy pipeline**\n",
        "\n",
        "How many named entities were found?   \n",
        "36.  \n",
        "*NOTE: To reduce user error I added a line of code to count the number of entities.*\n",
        "\n",
        "What types of entities appear? (PERSON, ORG, DATE, etc.)  \n",
        "The types of entities that appear are: ORDINAL, DATE, PERSON, GPE, ORG, CARDINAL, LAW, QUANTITY, PERCENT, WORK_OF_ART, and FAC."
      ],
      "metadata": {
        "id": "5Tr0_79fPQbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PhraseMatcher"
      ],
      "metadata": {
        "id": "LYeccjNQ5Xvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp2.vocab, attr=\"LOWER\")\n",
        "\n",
        "phrases = [\"new york\", \"life\", \"michelle choi\"]\n",
        "\n",
        "patterns = [nlp2(p) for p in phrases]\n",
        "\n",
        "matcher.add(\"TECH_TERMS\", patterns)\n",
        "\n",
        "matches = matcher(doc2)\n",
        "\n",
        "print(\"Matches found:\")\n",
        "for match_id, start, end in matches:\n",
        "    print(doc2[start:end])\n",
        "    print(\"Sentence:\", doc2[start].sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJdhOviq5bUn",
        "outputId": "d434368a-3b78-4d30-cb35-9d6446792803"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches found:\n",
            "life\n",
            "Sentence: If you\n",
            "ever wanted to start vlogging but you\n",
            "held back because you thought your life\n",
            "wasn't interesting enough, think again.\n",
            "\n",
            "life\n",
            "Sentence: This camera lets you\n",
            "capture your life through unique POV\n",
            "angles and tell stories in a way that\n",
            "feels creative, personal, and\n",
            "effortless.\n",
            "New York\n",
            "Sentence: My parents will be coming\n",
            "back and forth from New York and Korea\n",
            "and they'll be staying for like more\n",
            "extended amount of time.\n",
            "life\n",
            "Sentence: I think right now at\n",
            "this point in my life, I don't really\n",
            "care to have a fancy car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 7: Use PhraseMatcher**\n",
        "\n",
        "When you selected your YouTube video in Lab-1, what topic or subject were you interested in? Based on that topic, identify three specific phrases that are directly relevant to your search. For example, if your video was about information retrieval, relevant phrases might include \"information retrieval,\" \"text mining,\" and \"data science.\"\n",
        "\n",
        "When I selected my YouTube video in Lab-1, I was interested in videos by Michelle Choi about everyday life in New York. Three specific phrases that were directly relevant to my search were: \"new york\", \"life\", and \"michelle choi\"."
      ],
      "metadata": {
        "id": "OZugwl4NQ_Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 8: At the end of your Colab notebook, create a new text cell and write a brief reflection for this assignment in a few sentences addressing the following:**\n",
        "\n",
        "What went well?   \n",
        "Two things that went well were the overall **run** of code and the **change** of code. I never encountered any errors from the install to the phrase matcher. Also, when making changes, it was easy to know what to edit to fit the given prompt.\n",
        "\n",
        "What did not go well or what challenges you encountered?  \n",
        "Two challenges that I encountered were, when accessing the notebook in colab and describing outputs. When I tried to open the notebook from the link pasted on the Lab Assignment, it told me that the notebook could not be found. I realized that the address was false. The initial link said that the notebook was in Professor Lamba's main repository, but it was in a folder. Then, in terms of describing outputs, I struggled with this when asked about the entities. I had to re-read the question multiple times before I understood."
      ],
      "metadata": {
        "id": "qZzAnEV7ThaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXCERCISE\n",
        "\n",
        "Open a new Google Colab notebook and complete the tasks below. As you work, add brief explanations using the **Text (Markdown) cells** throughout your notebook to describe what you are doing.\n",
        "\n",
        "1. Make a new folder named `lab-2` in your `lis4693` or `lis5693` repo on GitHub. **[1 Point]**\n",
        "\n",
        "2. Complete the following tasks:\n",
        "\n",
        "\n",
        "**TASK 1**: Load and read your `transcript.txt` file from lab-1 from GitHub repo directly **[1 Point]**\n",
        "\n",
        "**TASK 2**: How many characters are in your transcript? Print the first 100 characters. **[1 Point]**\n",
        "\n",
        "**TASK 3**: Perform sentence segmentation using the blank pipeline\n",
        "  - How many sentences are in your transcript? **[0.5 Point]**\n",
        "  - Print the first 3 sentences **[0.5 Point]**\n",
        "\n",
        "**TASK 4**: Perform Word Count and Token Analysis\n",
        "  - What is the total number of words? **[0.5 Point]**\n",
        "  - What is the number of unique words? **[0.5 Point]**\n",
        "\n",
        "**TASK 5**: Find Most Frequent Words\n",
        "  - What is the most frequent word? **[0.5 Point]**\n",
        "  - Why do you think this word appears frequently? **[1 Point]**\n",
        "\n",
        "**TASK 6**: Run full spaCy pipeline\n",
        "  - How many named entities were found? **[0.5 Point]**\n",
        "  - What types of entities appear? (PERSON, ORG, DATE, etc.) **[0.5 Point]**\n",
        "\n",
        "**TASK 7**: Use PhraseMatcher **[1 Point]**\n",
        "\n",
        "When you selected your YouTube video in Lab-1, what topic or subject were you interested in? Based on that topic, identify three specific phrases that are directly relevant to your search. For example, if your video was about information retrieval, relevant phrases might include \"information retrieval,\" \"text mining,\" and \"data science.\"\n",
        "\n",
        "Make sure the video you selected in Lab-1 was clearly related to your chosen topic and was the same video for which you downloaded the transcript in Lab-1.\n",
        "\n",
        "**TASK 8**: At the end of your Colab notebook, create a new text cell and write a brief reflection for this assignment in a few sentences addressing the following **[2 Points]**:\n",
        " - What went well?\n",
        " - What did not go well or what challenges you encountered?\n",
        "\n",
        "3. Push your Google Colab file to your `lab-2` GitHub repo from Colab. *No points will be given if you upload it to GitHub directly!* **[1 Point]**\n",
        "4. Share the link to your `lab-2` GitHub repo for this lab assignment on CANVAS for credit. **[0.5 Point]**\n"
      ],
      "metadata": {
        "id": "gl10QFLRzuy2"
      }
    }
  ]
}